---
title: Streaming
sidebarTitle: Streaming
layout: ../_core/DocsLayout
category: Learn
sublinks: Introduction, Features, Advanced
permalink: /learn/streaming/
next: /learn/caching/
---

# Streaming

## Introduction

While Batch processing is need for some cases, such as gathering and data enrichment, there are other cases where the data is generated continuously, which typically send in the data records simultaneously. Streaming data includes a wide variety of data such as log files generated by customers using your mobile or web applications, eCommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services telemetry from connected devices or instrumentation in data centers.  
HKube's data streaming is an extension to HKube batch processing pipeline architecture that handles millions of events at scale,
In real-time. As a result, you can collect, analyze, and store large amounts of information.
That capability allows for applications, analytics, and reporting in real-time.

![StreamingDiagram](../../img/streaming/Streaming-diagram-nobackground.png)

### Use Cases - Stream Tweets in real-time

So where are HKube data streams good for? Let's take a look at a stream from Twitter as an example. In this particular case, we want to enrich the data from other resources, such as Facebook, LinkedIn, and other internal databases before saving it.
The process is as follows:

- Stream Tweets in Real-Time: Use the Twitter API and HKube Streaming to get and analyze real-time tweets for your research.
- twitt node: The "twitt" node subscribes to the Twitter API and receives real-time tweets.
- sort node: The "sort" node sorts the tweets based on their language and routes them accordingly.
- Nodes "A" and "B" analyze the tweet messages and save them to the database.

## Features

HKube streaming pipeline supports:

### Unique data transportation

HKube has its own data transportation system, enabling direct data transfer between nodes in a manner that ensures the following

- The data will maintain its order.
- High throughput with low latency.
- High scalability.

***

### Autoscaling

The throughput of streaming can vary over time, allowing us to handle bursts and free up resources for other jobs when they are not needed.
With its own unique heuristic system, HKube is able to recognize changes in throughput and act quickly to support these needs.
To better understand this, let's look at a scenario that demonstrates how HKube handles pressure.

#### Scaling Range

Autoscaling is used by each node by adjusting the number of pods it uses within the specified min-max range.
By using autoscaling, we shut down unnecessary pods to maintain efficiency.
HKube allows to adjust the minimum and maximum number of pods for stateless node:

**Minimum**:

- Defines minimum number of pods for stateless node.
- When the application starts, the stateless node will start at application up-time.
- Number of running pods can't be lower than the minimum number pre-defined.

**Maximum**:

- Defines maximum number of pods for stateless node.
- Number of running pods can't be higher than the pre-defined maximum.

***

### Conditional Data Flows:

In streaming data, most of the time, we expect the data to follow a specific flow. However, there are scenarios where we need to dynamically change this flow. To illustrate, let's consider the Twitter use case: typically, we aim to enrich the data with additional information from other resources. However, there may be instances where we are unable to identify the author of a post. In such cases, we need to establish additional prerequisites before proceeding with the enrichment. HKube assists in handling such situations with conditional data flow. We will explain how to create and work with this feature later.

### How to Work with Streaming Pipelines

Streaming pipelines are built from Stateful and Stateless algorithms.

#### Stateful Algorithm:

- A stateful algorithm is tailored for a specific execution.
- The stateful algorithm must use HKube's SDK to decide on which flow the data should continue: the default one or one of the conditionals.
- Only one stateful algorithm can be executed for each algorithm kind in a specific execution.
- There can be multiple stateful algorithms with different node names.
- The stateful algorithm will be closed if one of the following occurs:
	- The execution is terminated by the client using HKube's API.
	- The main function of the algorithm returns.

#### Stateless Algorithm:

- A stateless algorithm can serve multiple executions throughout its life.
- The stateless algorithm can dynamically scale (up/down) according to the jobâ€™s needs.
- Upon failure, HKube will skip the current execution for performance reasons.
- The stateless algorithm doesn't need to use HKube's API to continue the flow; this happens after the return command.
- Scaling up stateless algorithm depends on:
	- The sending node's queue size.
	- The rate of increase/decrease in queue size.
	- The processing time of the receiving node.

#### Streaming Flow:

- The flow represents the movement of data through the pipeline nodes (The flow must start with a Stateful Node/Algorithm).
- Streaming flows are defined in a simple syntax example: node A streams to nodes B and C, node B streams to node D. The syntax would be: A >> B&C | B >> D.

```json
{
    "streaming": {
        "flows": {
            "analyze": "sort>>A",
            "master": "twitt >>sort>>B"
        },
        "defaultFlow": "master"
    }
}
```

### Advanced

#### HKUBE API STREAMING METHODS for Stateful Algorithm

- **sendMessage(msg, flowName)**
    - This method passes on a message to the next node in the pipeline flow.
    - Parameters:
        - msg: A created message as desired to be obtained by the next node.
        - flowName: The name of the flow of nodes defined in the pipeline definition. This parameter should be given only if a new flow is initiated (not if the processing is already in the middle of a flow). If no flow name is given and the node is not in the middle of a flow initiated earlier, the default flow defined in the pipeline definition will be used as a flow name.
- **registerInputListener(onMessage=handleMessage)**
    - This method is used only within a stateful algorithm. It allows registering a method written by the algorithm implementor, which will be invoked upon each message that arrives.
    - The onMessage signature is onMessage(msg, origin), where the origin is the name of the previous node.

- **startMessageListening()**
    - This method is used only within a stateful algorithm. Once all message handlers have been registered using registerInputListener, startMessageListening needs to be invoked to start receiving messages upon arrival.